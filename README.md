# Today-I-Learned
 
* Deep Learning Seminar *(20.01.10 ~ )*
  + [1.17 notes](https://github.com/SoYoungCho/What-I-Have-Learned/blob/master/Deeplearning%20Seminar/1.17.ipynb)
  + [1.17 Bayes theorem, MLE & MAP PDF](https://github.com/SoYoungCho/What-I-Have-Learned/blob/master/Deeplearning%20Seminar/1.17%20%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88%20%EC%B6%94%EB%A1%A0%2C%20MLE%20%26%20MAP.pdf)
  + [1.17 Adding Bias term PDF](https://github.com/SoYoungCho/What-I-Have-Learned/blob/master/Deeplearning%20Seminar/1.17%20Bias%20term%20%EC%B6%94%EA%B0%80.pdf)
  + [1.20 notes](https://github.com/SoYoungCho/What-I-Have-Learned/blob/master/Deeplearning%20Seminar/1.20.ipynb)
  + [1.20 multi-variable PDF](https://github.com/SoYoungCho/What-I-Have-Learned/blob/master/Deeplearning%20Seminar/1.%2020%20x%EA%B0%80%20%EC%97%AC%EB%9F%AC%20%EA%B0%9C.pdf)
  + [1.22 Polynomial Regression & Overfitting](https://github.com/SoYoungCho/What-I-Have-Learned/blob/master/Deeplearning%20Seminar/1.22%20Polynomial%20Regression%20%26%20Overfitting.md)
  
* Neural Machine Translation (Capstone Project) *(20.01 ~)*
  + Intro to NMT
    + 1.25 [towards data science - NMT guide](https://towardsdatascience.com/neural-machine-translation-15ecf6b0b) : seq2seq + attention 이론부분까지 공부함. one-hot-encoding, cross entropy loss.
    + 1.26 [NMT mechanics of seq2seq + attention](https://nlpinkorean.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) : 시각화 잘 되어 있음. 번역 ver.
   
  + RNN
  + LSTM
  + GRU
  + seq2seq
    + 1.25 [Sooftware blog - seq2seq](https://blog.naver.com/sooftware/221784419691)
   
  + teacher forcing
  + attention
    + 1.25 [Sooftware blog - Attention Mechanism](https://blog.naver.com/sooftware/221784472231)
   
  + beam search
  + BLEU
 
