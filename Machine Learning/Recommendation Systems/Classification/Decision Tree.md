# Decision Trees

- 학습 데이터를 이용해 분류 규칙을 찾아내어 **트리 기반** 분규 규칙을 만드는 것
- 어떤 기준을 바탕으로 규칙을 만들지가 관건
	- 트리의 노드 : 규칙 조건
	- 트리의 leaf node : 결정된 클래스 값
	- 규칙이 많다는 것은 트리가 복잡함을 의미 -> 과적합 위험
	- 즉, 트리가 깊어질 수록 (depth) 예측 성능이 저하될 가능성이 높다
---
- 노드의 규칙(rule)은 최대한 많은 데이터 세트가 해당 분류에 속하도록 정해져야. -> 트리 split 기준 어떻게?
- 정보 
	- 균일도가 가장 높아지는 조건 선택
	- 균일도 = entropy 개념
	- 균일도 측정 방법
	- 1) 엔트로피 이용 Information Gain
	- 2) 지니 계수
		- 0~1
		- 1로 갈수록 불평등
		- 지니계수가 높은 속성 기준으로 분할
---
### 특징
- 장점
	- 균일도 개념 기반으로 하여 쉽고 직관적
	- 룰이 매우 명확
	- 어떻게 node와 leaf가 만들어지는지 알 수 있음
	- 시각화 표현 가능
	- 피처 스케일링 / 정규화 등 사전 가공 영향도가 크지 않음
- 단점
	- 과적합
		- 서브트리를 계속 만들면
		- 피처 많고, 깊어지고 복잡.
	- 해결책
		- 트리 크기 사전에 제한
---
### 파라미터
- ```min_samples_split``` : internal node를 split 하기 위한 최소한의 샘플 데이터 수, default = 2. 작을 수록 오버피팅 위험
- ```min_samples_leaf``` : leaf가 되기 위한 최소한의 샘플 데이터 수 / 비율, default = 1. float 시 해당 비율만큼 반올림 한 수
- ```max_features``` : 고려할 피처들의 최대 개수 / 비율 등 (int, float or {“auto” = “sqrt”, “log2”}). default = None, 데이터 세트의 모든 피처 사용해 분할 진행
- ```max_depth``` : 트리의 최대 깊이 규정. default = None. None으로 줄 경우 min_samples_split 파라미터에 맞춰서 끝없이 split 될 수 있으므로 오버피팅 방지 위해 제어 필요
- ```max_leaf_nodes``` :  leaf 최대 개수. 지정할 경우 균일도 기준으로 트리 구성, default = None. 
