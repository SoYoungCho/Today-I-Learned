# 9. 증거와 확률

- 증거 : 데이터 객체에 대해 알고 있는 것들, 타겟값에 도달하게 만든다
- train 데이터에 있는 어떤 증거가 가진 증거력 (Strength of evidence)

## 9.1 예제 : 온라인 고객 광고 타겟팅

- 과거 방문한 사이트 기반하여 고객에게 광고 타겟팅
- 사용자가 입력하는 문장 대신 다른 정보를 기반으로 유추해야
	- 예) 페이스북에서 사용자가 누른 좋아요 -> 관심의 증거로 사용 가능
- 여기서 객체 : 온라인 고객
- 타겟 변수 : 고객이 호텔 광고 본 뒤 1주 안에 예약
- 브라우저 쿠키를 통해 어떤 고객이 방 예약했는지 알 수 있다
- 모델 훈련 전
	- 각 고객마다 타겟 변수에 2진 값 저장
	- 광고를 본 고객이 방을 예약할 확률을 추정
	- 예산 한도 내에서 예약할 확률이 높은 고객부터 타겟팅
		- 고객 설명하기 위해 어떤 특징 사용 가능할까?
		- 이 예제에서는 고객이 이전에 읽은 블로그를 컨텐트 조각으로 정의, 이로 고객 설명
		- but! 본 예제에서는 이 가정에 의존하지 않는다 (고객이 검색하고 본 블로그를 이용하여 증거로 사용하지 않음)
		- 이력 데이터를 이용해 증거가 제시하는 방향과 증거력을 측정해보자.. (아직 무슨 말인지 잘 모르겠음)

## 9.2 증거의 통계적 조합

- 증거 E : 어떤 고객이 방문한 웹사이트들.
- 증거 E에 대한 C의 확률 = P(C|E)
- = E가 있을 때 C의 확률 = E 조건 하에서 C의 확률
- = 조건부 확률
---
* 독립이라는 것은
	* 고객들은 방문 기록이 다 불규칙하다
	* 따라서 증거 집합 E에 대해서도 동일한 E 집합이 충분치 않음
		* 즉, 각 증거 조각을 따로따로 생각하고 증거를 조합해야 한다.
		* 이 따로따로 생각한다는 것이 **독립을 가정하는 것임**!! 나이브 베이즈의 나이브가 이 의미

### 9.2.1 결합확률과 독립

- 두 사건이 모두 일어날 확률 P(AB) = 결합확률 (Joint Prob)
- 두 사건이 독립적일 때에는 이 결합확률은 p(A) * p(B)
- 두 사건이 종속적인 경우?
	- 조건부 확률 이용해 구할 수 있다
	- 이 때 결합확률 p(AB) = p(A) * p(B|A)

### 9.2.2 베이즈 규칙

![](https://lh3.googleusercontent.com/proxy/jE7Hk0vtMKsFnFhAmVcNdVOyLPYHBCzuU8zSOqWq8pXFsZgL2sKhC6uFQoTBrb-D5AVkcDEJxhnePhHBvU8dTGCyTq_lUMaalGeNhvMz3FT0hT23G3zaZzdwDXivLoOlhXfMJJQzmqdJ5xNByx77uia7xTtnMBUD7CIeGVv7O8qOaGe-vrmI1v43lNfHFThQa5vsZYsPDzh3x47wGawUTRTyTbldj0Udp8rQx_b89nBWkKGL30jNR0nn4F6dLT0hz_iPTaNtvYeh8ajIu3Ro4WqTAgDF69yr)

B자리에 우리가 아는 증거 E
A자리에 평가하고자 하는 가설 H 가 들어오면
- 증거 E를 조건으로 한 가설 H의 확률을 계산할 수 있다
- 이 확률을 구하는 것보다 나머지 확률 3가지를 구하는 것이 더 쉽다는 점 이용

## 9.3 데이터 과학에 베이즈 규칙 응용

- 분류에서의 베이즈 규칙
위 식에서 가설 자리에 구하고자 하는 타겟 변수 C가 오는 것(p282)

![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcS6FyFos1kqnsE_jsPx0pCNhFhuszDc5guh_A&usqp=CAU)

우리 데이터 마이닝 예제를 위 식에 적용하면, 증거의 집합인 P(E) - 위 그림에서는 P(B)
위 수식을 적용하기 위해선 모든 증거들의 결합확률?을 알아야하는데, 사실상 이는 불가
- 따라서 확률적 독립을 가정해 문제를 해결

### 9.3.1 조건부 독립과 나이브 베이즈

- 책 참고. p 284부터
- 핵심은, 각각의 증거들이 독립적이라고 가정하는 것

### 9.3.2 나이브 베이즈의 장단점

#### 장점
1. 간단한 분류자
2. 증거를 나타내는 feature vector (특징 벡터)를 계산에 모두 포함할 수 있음
	- 이는 저장 공간, 계산 시간 측면에서 매우 효율적
	- 이유 : 훈련하는 동안 사례를 조사하며 **단지 계층과 특징이 나타나는 횟수만 저장하면 되기 때문**
	- p( c ) 의 경우 전체 사례에서 c계층이 속한 사례의 개수를 비율로 환산해 추정
	- p( e | c )의 경우 c 계층에서 e 특징이 나타나는 사례 수의 비율로 추정
	- 단순, feature 간 엄격한 독립성 가정
	- 두 증거 간 의존성이 상당히 강한 경우에도 큰 상관이 없다
		- 두 증거를 이중적으로 계산하는 격이지만, 이중 계수는 확률 추정치를 약간 더 크거나 작게 할 뿐 문제가 되진 않음.
		- but, 확률 추정치 자체를 사용하려는 경우에는 문제가 될 수
			-  나이브 베이즈를 이용해 비용&효과 계산해 의사 결정하는 경우 주의 필요
			- 객체 정렬할 경우 실제 확률값 사용하는 대신, **객체가 다른 계층에 속할 가능성을 상대적으로 비교하기 때문**
3. 점진적 학습자 (incremental learner)
	- 한번에 사례 하나씩 학습하며 모델 갱신
	- 새로운 훈련 데이터 추가 되면 과거 학습 사례는 다시 학습할 필요 없음
		- 이를 이용하면, 앱 실행 도중 데이터 레이블을 알게 되어 모델에 빠르게 반영하고자 하는 경우 유용

## 9.4 증거 '향상도' 모델 (A Model of Evidence “Lift”)

- Lift (향상도)
	- 분류자가 negative 음성 객체보다 positive 양성 객체를 얼마나 많이 모았는가를 나타냄.
	- 전체 모집단에 대해 **모델이 선택한 부분 모집단 중 positive 양성 객체의 비율** 측정
- 나이브 베이즈를 수정해 **증거별 향상도를 모델링 후 간단히 결합**
- 나이브 베이즈 -> 조건부 독립의 약한 가정
- 향상도 모델링 하도록 수정 -> 각 항목들이 완전히 독립이라고 가정해야
- lift 공식은 책 참조 (287)
---
- 새로운 사례를 분류할 때?
	- 각 증거 e는 자신의 향상도에 따라 계층에 속할 확률을 높이거나 낮춤

## 9.5 페이스북 좋아요의 증거 향상도

- 페이스북 좋아요를 누른 것과 이에 대한 지능 향상도에 대한 연구 결과
- 표본의 양성 비율 14%면 기준율이 14%라고 생각할 수 있음
- 만약 향상도가 1.3인 게시물 좋아요 누르면 지능 IQ 추정확률은 0.14 * 1.3 = 18%가 된다

## 요약

이번 장의 핵심은 '각 타겟이 특징값을 어떻게 생성하는가?'
“How do different targets generate feature values?” 
-> 어느 계층이 이 사례가 가진 특징을 만들 가능성이 높은가?

- 나이브 베이즈 : 타겟에 미치는 각 특징의 영향을 독립적으로 모델링
- 특징 간의 상호 작용을 고려할 수 없음 (naive)
- 단순, 빠르고 효율적, 효과적
- 특징 간 독립을 가정하면 결론에 영향을 미칠 수 있는 증거를 조사하기 위한 '증거 향상도'를 쉽게 구할 수 있음
