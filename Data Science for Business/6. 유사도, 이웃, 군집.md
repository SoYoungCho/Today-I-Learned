
### 6.2.5 최근접 이웃 방법의 문제점 (KNN)

#### (1) 모델 명료성 (intelligibility)
- 최근접 이웃 모델 (nearest-neighbor methods) 은 일반적으로 이해하기 어려움.
- 모델 명료성에는 크게 두 가지 측면 있음
	
	a. 결정에 대한 정당성
	- 예) 주택 담보 대출 가능 고객? 아닌 고객? -> 고객 데이터셋에서의 유사도 계산을 진행하여 판단한다? 	당신은.. 대출금을 갚지 못한 홍길동씨를 떠올리게 하므로 대출 안해드릴거임 - 이럴 수 없음

	b. 전체 모델의 명료성
	- 어떤 '지식'을 데이터 마이닝하였는지 깊이 있게 설명하기 어려움
	- 차원이 높아질 수록 시각적 표현 어려움

#### (2) 차원 및 영역 지식 (dimensionality and domain knowledge)
- 객체 거리 계산 시 객체의 feature 모두 고려
- 수치형 속성의 경우 : 넓은 범위에 거쳐 있으므로 (wide range) 범위가 작은 속성은 너무 큰 영향을 받는다. 
- 피처가 너무 많거나 유사도 판단과는 무관한 쓸모 없는 피처가 많은 것도 문제가 될 수 있음

-> 차원이 높아서 발생하는 문제 : 차원 수의 저주 (curse of dimensionality)
	= 거리 계산 시 모든 피처를 포함시키면 관련 없는 속성들이 객체에 너무 많은 영향을 주어 유사도 측정 올바르게 못하게 될 수.
- 이 문제를 해결하는 여러 방법 : feature selection (특징 선택)
	- 특징을 신중하게 결정해 데이터 마이닝 모델에 포함할지를 결정 (수동. 직접)
	- 3단원, 5단원에서 배운 것처럼 자동화된 피처 선택도 가능 (target에 대한 정보를 많이 제공하는 피처)
- 유사도 구하는 함수 / 거리 구하는 함수를 튜닝하여 (수작업 조정) 유사도 계산에 도메인 지식 적용 가능
	- 피처 웨이트 조정

#### (3) 계산 효율성 (computational efficiency)
- KNN 모델 장점은 모델 학습이 빠르다는 것 (그냥 데이터 저장하기만 하면 되므로)
- 하지만 이 모델의 주된 계산 (연산)은 예측 및 분류에서 일어남. (가장 가까운 객체 찾기위해 데이터베이스 검색해야)
- 계산을 빨리 해야하는 경우 (온라인 타겟 광고) NN 사용하기 어려움 

## 6.3 유사도 및 이웃에 관한 주요 세부 사항

### 6.3.1 이질적인 속성 (feature scaling에 관하여)

- 피처(속성)가 연속적인 수치형일 경우 거리 계산은 간단.
- but, 복잡하고 이질적인 속성이 잇을 경우 계산은 복잡해짐.
	- 예) 나이의 경우 연속형. 수치형. 그런데 성별 속성의 경우 분류 (기호) 속성이므로 수치형으로 변환해야. 
	- 수치형 변수라고 해도, 규모와 범위가 서로 다른 점에 주의해야.
		- 예) 나이 - 최대 100. 소득이라면? 100억까지 들어갈 수.
		- 이러한 경우 속성 간 규모를 맞추지 않으면 (feature scale을 맞추지 않으면) 소득에서 10원 차이 = 나이 10살 차이.
		- --> 따라서 변수 값의 규모를 조정해야 한다.
		- 변수가 가질 수 있는 값의 범뤼를 측정, 이에 따라 규모를 조정하거나 어떤 고정된 개수의 항목에 배분한다. (10개의 항목으로 자르는 등)

### 6.3.2 그 외 거리 함수들 
	- 거리는 작을 수록 좋다는 사실 기억!

* 유클리드 거리
	* 각 차원에서 거리의 제곱 이요해 계산
	* L2 Norm (노름)
	* 흔히 아는 피타고라스 그 공식

* 맨해턴 거리 (맨하턴 거리, Manhattan distance)
	* L1 Norm
	* 각 차원의 거리의 합 (절댓값, 절대값)
	* 각 차원에서의 차이를 합함. 동서 - 남북 처럼 이동

* 자카드 거리 (Haccard distance)
	* 두 객체를 특징 집합으로 처리. 집합의 개념
	* 객체를 하나의 집합으로 생각
		* 두 집합의 공통된 특징의 집합 (교집합)
		* 즉, 자카드 거리는 두 객체가 공유하는 모든 특징에 비례
		* 두 객체 모두에 있는 특징은 중요 (두 객체 중 하나에만 있는건 중요하지 않음)
		* 예) 위스카의 경우, 둘 다 연기 맛이 난다는 특징은 중요. 둘 다 짠맛이 나지 않는다는 건 중요하지 않음.

* 코사인 거리
	* 코사인 유사도 개념과 동일. 코사인 거리 = 1 - 코사인 유사도 
	* 장점 : 객체 간의 크기 차이를 무시하고자 할 때. 다시 말해, 벡트의 크기를 무시하고자 할 때.
	* 각도의 개념이기 때문. 
		* 예) 텍스트 분류시 텍스트 내용 자체에 집중. (텍스트 길이 말고)
		* 단어 a가 1번, b가 2번, c가 3번 나온 문서와 10,20,30 나온 문서의 코사인 거리는 0.

* 편집거리, 레벤쉬타인 거리.
	* 문자열 (텍스트)의 유사도를 측정할 때 사용할 수 있는 측정법
	* 편집 거리 : 글자 하나를 삽입, 삭제, 치환할 수 잇는 편집 연산이 몇 번 필요한지 최소 연산 횟수를 계산. 적을 수록 유사한 것.

### 6.3.3  Combining Functions: Calculating Scores from Neighbors (혼합 함수)

- 혼합함수 : 데이터 포인트의 nearest neighbor로부터 target 예측하는 데 사용되는 공식.

* 다수결 투표 분류 (majority vote)
	* k개의 이웃들의 클래스 개수를 세는 것. (KNN 처럼)

* 유사도 반영 분류 (similarity moderated)
	* 위 식에 가중치 개념을 포함.
	* 얼마나 많은 이웃이 얼마나 많은 영향을 미칠까?
	* 단순히 카운팅을 하는게 아니라 가중치를 주는 것.
	* 가중치 주는 방법 : 해당 이웃이 얼마나 먼지 가까운지를 반영. 가까울 수록 가중치 크게 주고, 멀 수록 가중치 낮게 줌. 
	* 가중치 = 1/ 거리^2

* 이 식에서 전체 거리 합으로 나누어주면 확률 추정에 사용 가능 (p.198)
* 이 식에서 클래스 대신 수치 값으로 바꾸면 회귀 분석에 사용 가능

## 6.4 군집화 (clustering)

- 감독 세분화 (supervised segmentation) : 관심을 갖고 있는 타겟 특징에 따라 그룹화될 수 있는 객체들을 찾아내는 것.
- 풀어 쓰면, 목표로 하는 클라스의 feature에 따라 **그룹화**, 묶일 수 있는 객체들을 찾는 것.
	- 이 때, 왜 '감독'이라고 하는 거지? 왜 지도 (supervised)?
	
- 군집화란? (=자율 세분화, unsupervised segmentation)
	- 데이터에서 자연스럽게 분류되는 그룹을 찾아내는 것
	- 기본 개념 : 그룹에 들어 있는 객체들은 비슷하지만, 다른 그룹에 있는 객체들과는 비슷하지 않도록 객체의 그룹 찾아내는 것 (intra distance, inter distance 그 개념 의미)

- 감독 모델링 vs 자율 모델링
	- 감독 모델링 (supervised modeling)
		- **타겟값을 알고 있는 데이터를 이용**해 새 객체의 타겟 변수값을 예측할 수 있는 패턴 찾는 것
	- 자율 모델링 (unsupervised modeling)
		- 타겟 변수에 신경 쓰지 않고, **데이터 집합에 있는 규칙성** 찾는 것

### 6.4.2 계층적 군집화 (Hierarchical Clustering)

- 최상위 수준의 군집 : 모든 데이터를 포함
- 최하위 수준 : 각 데이터
- 계통도 (dendrogram) 개념 - 하단부터 두 개씩 짝 지어 잇는 것. (x축은 데이터 군집, y축은 군집 간의 거리)
- 한 번에 여러 그룹으로 나누지 않고, 객체를 차근차근 그룹화하는 다양한 방법
	- 각 노드를 하나의 군집(cluster)라고 생각하고 군집화 시작함.
	- 그 다음, 하나만의 클러스터가 남을 때까지 반복해서 병합
	- 즉, 계통도를 수평으로 자르면 원하는 군집의 개수로 자를 수 있음.
	- 특히 계통도에서 수직선이 길다는 것은 거리가 길다는 의미이기에 수직선이 지나치게 길어지는 지점에서 클러스터 나눌 수 있다.
- 장점 : 추출한 군집의 개수를 결정하기 전에, 데이터 유사도의 지형 - 즉 어덯게 그룹들로 묶을 수 있는지 알게 해준다.
	- 다시 말해, 클러스터 개수를 몇 개로 할 것인지 정하는데 유용하다.

- 군집 간의 거리를 계산하는 함수 : Linkage Function (연결 함수)
	-  두 군집에서 **가장 가까운 점 간의** 유클리드 거리 등 다양.

- 군집과 유사도 순위가 정확히 일치하지 않는 이유? p 204 하단

### 6.4.3 최근접 이웃 : Clustering Around Centroids (중점으로 군집화 하기)

- 계층적 군집화 : 개별 객체 간 유사도 & 유사도가 객체를 서로 연결하는 방법에 중점을 둔다
	- 즉, 객체 간 유사도 (intra cluster minimize) & 클러스터 간 거리 계산 (inter cluster maximize) 의미인듯.
- 데이터의 군집화 (clustering) = 각 군집을 군집의 중점(centroid)로 표현하는 것이 군집에 집중하는 핵심

- Centroid : 클러스터 중심에 가장 가까운 점을 임의로 랜덤하게 선택

#### k-means 알고리즘 (k-평균)
- 여기서 말하는 means -> 중점
- 클러스터에 들어 있는 객체들은 각 차원별로 산술 평균 값으로 표현 된다
	- 모든 차원을 고려하여 하나의 데이터 포인트로 표현한다는 뜻 인듯하다.
	- 예를 들어, x 좌표는 모든 점의 x좌표 평균, y좌표는 모든 점의 y좌표 평균
	- 일반적으로 중점은 클러스터 내 모든 데이터 포인트가 가진 피처의 평균 값
- 여기서 말하는 k -> 찾고자하는 클러스터 개수
	- 아까 계층적 군집화 (Hierarchical )에선느 따로 클러스터 개수 정하고 시작한 것 아님.
	- 이와 달리 k-means는 원하는 클러스터 개수로 시작한다.
- k-means clustering을 통해 알 수 있는 정보
	- 1. k개의 군집의 중점 (centroid)
	- 2. 각 클러스터에 어느 점들이 속하는지
		- 위 정보에 의해 각 클러스터가 중점에 가까운 점들을 갖고 있기에, 최근접 이웃 군집화 (Nearest Neighbor Clustering)이라고 불리기도 한다.

* k-means 의 절차
1. k개의 centroid를 만든다.
		* 랜덤하게 선택되기도 하고, 실제로 선택하거나, 사용자가 설정하거나 전처리 하기도 함.
2. 각 점에서 가장 가까운 클러스터 centroid가 무엇인지 판단 -> centroid 중점 주위에 cluster 형성한다
3. 각 군집마다 실제 중점 다시 계산 -> centroid가 이동
4. 위 과정을 반복
	- (1) 중점이 이동하였기에 각 데이터 점마다 어느 클러스터에 속하는지 다시 계산해야 (데이터 점 단위의 cluster 계산)
	- (2) 각 점이 속한 군집을 다시 계산한 후에는 군집의 중점을 다시 계산 (centroid 계산)
5. 각 단계의 결과는 군집의 왜곡 측정으로 비교 간으 (distortion)
	- 왜곡 : 각 데이터에서 해당 중점까지의 거리 제곱의 합. 즉, intra cluster distance.
	- 왜곡값이 가장 작은 군집이 제일 좋음 (intra-cluster distance should be minimized)

* k-means 알고리즘 특징
	- k means 장점
		- 실행시간 측면에서 보면 효율이 좋음
		* 모든 데이터 간의 거리 계산이 아니라, 각 데이터와 centroid 간의 거리만 계산하면 되기 때문.
		- 반면, 계층적 군집화 (Hierarchical)  단점: 시작 단계의 모든 측정점의 쌍과 각 회차 마다 모든 군집 쌍 간의 거리를 알아야 하므로 일반적으로 느림

	- k 값을 잘 결정하는 것이 핵심.
		- 너무 작고 지나치게 세세한 군집이 있다면 k값을 줄인다
		- 너무 넓게 퍼진 군집의 경우 k값을 늘린다
		- elbow method : 안정화 되기 시작할 때의 k값을 최소값으로 선택하는 것이 좋다

### 6.4.5 군집화 결과의 이해

- 클러스터의 항목을 각 데이터의 이름으로 표현 가능하다는 점
	- 예) 편의점의 경우 각 손님들 이름은 딱히 쓸모 없지만, 기업의 경우 고객을 클러스터링 한다면 기업 고객의 이름들은 영업부서원, 관리자들에게는 유용.

- 군집의 대표적인 항목
	- 클러스터 내부에 객체가 많을 경우 유용


### 6.4.6 cluster 설명 위한 감독학습법 적용 (Using Supervised-Learning to explain clusters)

- 클러스터링 : 어느 군집에 어느 데이터가 포함되는지만 있다 (어떻게 군집화 했는지는 설명 불가)
- 군집의 중점 (centroid) : 군집의 평균 데이터 의미
- but, 위에서 말했듯, 클러스터들이 어떻게 다른지에 대해서는 설명하지 않는다.
- 우리가 원하는 것은 '왜 이 군집이 저 군집과 다를까'

--> 이를 위해 지도학습 (감독학습, supervised learning) 사용 가능
- 할당된 cluster를 일종의 label로 사용하는 것.
- 이렇게 label이 붙을 경우 지도핛브을 실행해 각 클러스터에 대해 분류자 생성 가능 (분류 모델 만들기 가능)
- 분류자 설명을 살펴 보면 해당 클러스터에 대한 알기 쉽고 간결한 설명 볼 수 있음 (differential description)
- k개의 클러스터가 있으므로 각 군집 vs 그 군집 제외 나머지 군집 비교 가능
	- 클러스터 A 인지 클러스터 A 아닌지와 같이 해서 decision tree 만들기 가능 (분류 파트에서 다룬 것 과 같이)
	
#### 특성 설명 vs 감별 설명
- 특성 설명
	-  (라포인트, 르존드라의 설명)
	- 이 클러스터가 다른 클러스터와 다른지 같은지에 대해서는 무시,  클러스터에 속한 데이터들의 전형적인 특성만을 설명
	- 그룹 간의 공통점에 초점
- 감별 설명 
	- 다른 클러스터와의 차이점만을 설명. 클러스터 내 다른 데이터들이 공통적으로 가진 특성은 무시.
	- 그룹 간의 차이점에 초점

- 두 방법 중 어느것이 더 뛰어나다고 할 수는 없지만, 이를 어디에 사용하는지에 따라 두 방법 중 하나를 선호할 수 있다

## 6.5 비즈니스 문제 해결과 데이터 탐사 문제

- 데이터 과학의 기본 개념 중 하나 : 데이터 마이닝 **목표를 가능한 정확히 정의**해야 하는 것
- CRISP 데이터 마이닝 프로세스 : 문제를 정의하기 전에 , 비즈니스와 데이터를 이해하는 데 시간투자 많이 필요.
- 즉, 타겟 변수를 정확히 정의해야한다

### supervised vs unsupervised
- 감독 문제 (지도학습 문제)
	- 해결하려는 문제 정의에 시간 투자
	- 평가 질문은 다음과 같이 됨 - 우리가 정의한 문제를 해결하는가?
	- 예) 고객 이탈율의 예측 정확도가 높아졌는가?
- 자율 문제 (비지도학습 문제)
	- 탐사적 성격이 강함
	- 데이터에서 무언가 중요한 것을 발견할 수 있는 기회
	- 데이터 마이닝 평가단계에서 창조성, 비즈니스 지식을 이용해 비지도학습 결과에 의미 부여해야.
	- 초기 데이터가 없을 때 비지도학습을 통해 얻은 인사이트를 데이터마이닝 프로세스에 적용하여 예측 모델링 문제 (지도학습) 정의, 예측 가능 사례 있음 (221쪽)

## 6단원 요약

- 유사도 : feature 벡터 표현으로 정의된 객체 공간에서 두 객체 간 거리 이용
- 최근접 이웃법
- 자율 데이터 마이닝 대표적 사례 - 군집화 (unsupervised data mining)
- 자율 문제 (unsupervised)의 경우 데이터 탐사 시
	- 초기 단계에선 시간 투자 줄이고
	- 평가 단계에 시간 많이 할애할 것
