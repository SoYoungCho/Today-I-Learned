## 1.28(화) 캡스톤 회의 결과 정리

장소 : 서울역 연세세브란스병원 던킨도너츠

### 내용

RNN, LSTM, seq2seq, seq2seq + attention에 대해 발표하고 이해가 안가는 부분에 대해 이야기 나누었다.

### 오늘의 A-HA!

* 기울기 역전파에서 왜 미분하는지 : 해당 값이 결과에 얼마나 영향을 미치는지 알기 위해
* LSTM에서의 gt (그림에서의 gt)가 무엇을 의미하는지 이해함.
* RNN과 달리 LSTM은 hidden state 뿐 만 아니라 cell state도 넘긴다.
* seq2seq의 인코더 디코더 내부의 cell로 LSTM을 쓰면 인코더에서 디코더로 hidden state vector (context vector) 뿐 만 아니라 인코더의 cell state도 같이 넘겨준다
* sigmoid 함수의 단점 : zero-centered가 아니여서 항상 0 - 1의 양수가 나오기에 출력값 weight가 갈수록 커져서 역전파 시 기울기가 0 / 1로 수렴하는 문제가 발생할 수  있다.
* seq2seq & attention 기법 + 입력 문장을 뒤집어 넣어주는 reverse 방법도 함께 적용할 수 있다. attention 기법을 같이 쓰면 효과가 미미할 것이라고 생각했는데, 물어보니 성능이 많이 좋아진다고 한다. 우리도 같이 써야겠다.
* attention 기법을 적용하는 경우 역전파는 어떻게 이루어지는지 궁금했는데, 어떤 attention 기법을 사용하는지에 따라 다르지만 단순히 내적하는 방식(dot)이라면 파라미터가 없기 때문에 기존 seq2seq에서의 backpropagation과 동일하다고 생각하면 된다. 파라미터가 있는 어텐션 기법을 사용하면 파라미터 학습을 위한 미분 필요.

### 여전히 모르겠는 것

* RNN 구현 코드를 보면 return dx, dh_prev 해주는데, 왜 dx도 반환하는지 모르겠다. dx가 쓰일 때가 있나?
* seq2seq에 들어가는 문장 길이가 다 다른데 내부적으로 어떻게 weight가 학습될까? 코드를 보면 더 이해가 갈 것 같아서 일단 pass.
* (위와 마찬가지) LSTM cell에 문장이 어떻게 들어가는지

### 향후 일정

다음 회의 : 목요일 12시 30분 (도서관)
다음 회의까지 해야 할 것 : 파이토치 홈페이지의 튜토리얼로 seq2seq train 전까지 코드 이해하고 두드려보기

#### 목요일에 교수님과 면담하며 여쭤볼 것.
- 바뀐 주제에 대해 말씀 드리기
- GPU 서버 사용 관련 문의.
- 데이터 개수 부족한지 (160만개)
- 리얼셀러와의 상담 결과 여쭤보기.

- (언젠가) 타과 교수님들께도 메일로 면담 요청 드려보기
