
### odds 
: 단순 확률의 비
binary의 경우 : 일어날 확률 & 일어나지 않을 확률
multi : 원하는 경우 & 특정 타겟

이를 그래프로 그리면 거의 0, 1에 가까워지면 지수적으로 증가.

### logit
: odds에 log를 붙여 확률의 비율을 표현.

 특징
 - -무한대 ~ 무한대의 범위를 지니는 것이 특징
- activation 함수를 지나지 않기에 때문에 뉴런의 반쪽 형태

### probability
: 전체 경우의 수에서 원하는 경우의 비.

---
softmax 동작 원리 : 식 유도

### cross entropy loss
-> nll (negative log likelyhood) + softmax

---

## MNIST Dataset

각 픽셀은 0~1까지 확률로 표현 됨.

레이어가 쌓일  수록 추상화 된다.

**오늘 수업 목표**

1. hidden layer 없이 input, output 레이어만 있으면 어떻게 될지 알아보자. (어떤 문제점이 있을지)
2. 딥러닝의 내부 과정을 어떻게 해석할 수 있을지 (forward, backward)
⇒ 왜 스택을 쌓는지 알아보자
---
## Convolution vs Correlation

CNN? 사실 convolution 말고 correlation이 되었어야.

- 뒤집고, 안 뒤집고의 차이 (180도)

#### 1. convolution 
: 어떤 신호가 통과했을 때, 신호를 **뒤집어서** 연산.
 
*참고!* 신호가 들어가면 그 신호를 impulse로 나누어 줄 수 있다고 함. 신호처리에서는 그 impulse를 곱한 값이 무엇을 닮았는지를 통해 해당 신호가 어떤 신호인지 판단. (예) 기지국에서 핸드폰으로 '카톡' 신호 보낼 때) 

#### 2. correlation 
: 두 신호가 얼마나 닮아있는지 비교. (뒤집지 않는다)
위치에 맞는 것(impulse)끼리 곱한 값이 크면 유사하다고 판단. (닮아있다고). 필터를 element-wise하게 곱해준다

즉, 자기의 필터를 그냥 곱해주는 것. (뒤집는 과정이 없다)

* 그런데 영상처리 에서는 필터를 180도 뒤집더라도 뒤집힌 필터는 대부분의 경우 동일하기 때문에 그냥 convolution이라고 말하는 것.

---
수학적 식을 통한 유도. (forward, backpropagation)

--- 

### input, output 레이어만 가지고 필터를 만든 결과
--> 필터가 0~9를 나타내지 못하고 있다.
사람마다 쓴 글씨가 다르기에 general 한 필터를 만들 수 없다

#### bias 의 역할 : 영상보고 다시

---
즉, 10개의 클래스로 분류한다 -> 10개의 필터를 사용.
correlation도 범위가 -무한대 ~ +무한대.
이를 logit으로 하여 여기에 softmax 등 해서 하는 것

but, 필터 모양을 보면 알 수 있 듯, 
input, output 레이어만 가지고 하는 데 에는 한계가 있음.
