# Cross Entropy

지금까지는 prediction에 MSE (mean squared error) 사용 했었음.

sigmoid를 하면 0~1. 이렇게 결과가 나오는데
이 때 가장 적합한 loss function이 Cross Entropy Loss
확률적으로 보았을 때는 Cross Entropy Loss가 더 좋다.

## Information Gain

 high randomness = high entropy.
 ex) abcdefg -> 1/6. ⇒ higher randomness. 예측하기 어렵다.
	 aaabbb -> 1/2 


Information graph = $-log(x)$
정보이론에서 
정보 = 놀람의 정도 = 확률이 낮은 event
 
ex) 카드 1-100장 중 랜덤하게 뽑은 한 장을 맞춰라
정보1: 정답은 1~100 사이
P(x) = 1 -> 확률은 당연히 1. 너무 당연한 말. 적은 정보
정보2: 정답은 5번 카드
P(x) = 0.01 -> 정보가 더 많이 담겨 있다.

확률에서 다루는 Mean (expectation value)
$E(x) = \Sigma (p * f(x))$

엔트로피 : 평균 정보량

불확실정도가 높다 (randomness)
= 평균정보량이 높다
= 엔트로피가 높다

### KL Divergence
두 확률분포가 **얼마나 다른지** 수치화 하는 방법

두 분포 그래프 p와 q가 얼마나 다른지 수치화 한다면, 어떻게 할 수 있을까?

어떤 점 $x_i$ 에 대한 두 그래프의 값 차이를 비율을 구하면 다음과 같다. 

$q(x_i) / p(x_i)$ 
(-> 기준은 분모에 온다)

이것의 정보량을 구하면?

$-log$  ($q(x_i) / p(x_i)$)

만일 두 분포가 동일하면 $q(x_i) / p(x_i)$ = 1.
$-log$  ($q(x_i) / p(x_i)$) = 0

따라서 놀람의 정도는 0. 

---
평균값을 구하는 식 : $\Sigma p(x_i) f(x_i)$

이 식에서 $f(x_i)$ 이 자리에  $-log$  ($q(x_i) / p(x_i)$).
즉, f 자리에 두 분포의 값 차이의 비율의 정보량을 취한 것이 KL Divergence 구하는 식. 다음과 같다.

$D_{KL} (p||q) = \Sigma p(x_i) -log$  ($q(x_i) / p(x_i)$)

즉, 처음에 **두 분포 차이의 비율**을 **정보량**으로 바꾸고,  정보량의 **평균값**.
 **엔트로피를 구하는 방법으로 똑같이.**
왜 이렇게 엔트로피 비슷하게 구할까? 엔트로피와 똑같은 form, dimension을 가지기 위해서. (노타빌리티에 필기했으니 참고)

* 0~1에 -log 취하면 정보량이 됨.
---
### Cross Entropy
* 정보 : 확률이 낮을수록 큰 정보량
* 엔트로피 : 평균 정보량
* KL divergence : 두 확률 분포가 얼마나 다른지.
**KL divergence = Cross Entropy. 동일한 개념**
KL Div.는 자신의 엔트로피를 제외한 크로스 엔트로피와 똑같다.

Cross Entropy도 결국 분포의 차이 비교하는 것.
KL Divergence  와 동일한 식이다.

## Cross Entropy for Binary Classification

타겟의 prob dist.이 0 또는 1이 된다는 것이 특징

mse 에서도 차이가 커질수록 mse 값이 커지게 했음.
BCE에서도 마찬가지이다.
레이블이 0/1에 따라 두 항 중 하나만 남고 (0이 곱해지니)
theta 레이블에 따라서 theta 예측에 페널티를 준다.
(레이블이 0이면 예측이 1에 가까워질수록 패널티가 크다. -log(x) 그래프로 그린거니까.)
